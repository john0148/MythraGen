pretrained_model_name_or_path = "ckpt/v2-1_768-ema-pruned.safetensors"
caption_extension = ".txt"
resolution = "768,768"
cache_latents = true
enable_bucket = true
bucket_no_upscale = true
output_dir = "data/lora_sdscipts_example_data/models"
output_name = "sunflower"   # name of saved LoRA weights
save_precision = "bf16"
save_every_n_epochs = 1
train_batch_size = 2
max_token_length = 255
xformers = true
max_train_epochs = 10
persistent_data_loader_workers = true
gradient_checkpointing = true
mixed_precision = "bf16"
logging_dir = "data/lora_sdscipts_example_data/logs/"
sample_every_n_epochs = 1
sample_prompts = "data/lora_sdscipts_example_data/val_prompt.txt"
sample_sampler = "ddim"
optimizer_type = "AdamW8bit"
learning_rate = 0.0001
lr_scheduler = "cosine_with_restarts"
lr_warmup_steps = 500
lr_scheduler_num_cycles = 3
dataset_config = "data/lora_sdscipts_example_data/dataset_config.toml"
unet_lr = 0.0001
text_encoder_lr = 5e-5
network_module = "networks.lora"
network_dim = 8
network_alpha = 1